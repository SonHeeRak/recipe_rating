{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_interactions = pd.read_csv('./data/RAW_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_rating = raw_interactions[['review', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great with a salad. Cooked on top of stove for...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So simple, so delicious! Great for chilly fall...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This worked very well and is EASY.  I used not...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I made the Mexican topping and took it to bunk...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Made the cheddar bacon topping, adding a sprin...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating\n",
       "0  Great with a salad. Cooked on top of stove for...       4\n",
       "1  So simple, so delicious! Great for chilly fall...       5\n",
       "2  This worked very well and is EASY.  I used not...       4\n",
       "3  I made the Mexican topping and took it to bunk...       5\n",
       "4  Made the cheddar bacon topping, adding a sprin...       5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# review data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1132367 entries, 0 to 1132366\n",
      "Data columns (total 2 columns):\n",
      "review    1132198 non-null object\n",
      "rating    1132367 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 17.3+ MB\n"
     ]
    }
   ],
   "source": [
    "review_rating.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review    169\n",
       "rating      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_rating.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# review_rating data have null data in 169 review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_rating = review_rating[review_rating.review.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    816229\n",
       "4    187333\n",
       "0     60847\n",
       "3     40852\n",
       "2     14122\n",
       "1     12815\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_rating.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# review_rating data have 0 in 60847 rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_rating = review_rating[review_rating.rating != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f6b4904e48>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_rating.rating.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = review_rating[review_rating.rating == 5]\n",
    "review_rating = review_rating[review_rating.rating != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816229"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_4_num = len(review_rating[review_rating.rating == 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So simple, so delicious! Great for chilly fall...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I made the Mexican topping and took it to bunk...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Made the cheddar bacon topping, adding a sprin...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Better than the real!!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Absolutely AWESOME! I was speechless when I tr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132359</th>\n",
       "      <td>This is the best and easiest hot fudge ever. I...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132360</th>\n",
       "      <td>Delicious quick thick chocolate sauce with ing...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132363</th>\n",
       "      <td>These were so delicious!  My husband and I tru...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132364</th>\n",
       "      <td>WOW!  Sometimes I don't take the time to rate ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132366</th>\n",
       "      <td>I am so glad I googled and found this here. Th...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>816229 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    review  rating\n",
       "1        So simple, so delicious! Great for chilly fall...       5\n",
       "3        I made the Mexican topping and took it to bunk...       5\n",
       "4        Made the cheddar bacon topping, adding a sprin...       5\n",
       "9                                   Better than the real!!       5\n",
       "10       Absolutely AWESOME! I was speechless when I tr...       5\n",
       "...                                                    ...     ...\n",
       "1132359  This is the best and easiest hot fudge ever. I...       5\n",
       "1132360  Delicious quick thick chocolate sauce with ing...       5\n",
       "1132363  These were so delicious!  My husband and I tru...       5\n",
       "1132364  WOW!  Sometimes I don't take the time to rate ...       5\n",
       "1132366  I am so glad I googled and found this here. Th...       5\n",
       "\n",
       "[816229 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = random.sample(temp.values.tolist(), rating_4_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(temp, columns=['review', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_rating = pd.concat([review_rating, temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f6085586d8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_rating.rating.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442455"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_rating = pd.read_csv('data/review_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great with a salad. Cooked on top of stove for...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This worked very well and is EASY.  I used not...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>very very sweet. after i waited the 2 days i b...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This recipe was OVERLY too sweet.  I would sta...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very good!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating\n",
       "0  Great with a salad. Cooked on top of stove for...       4\n",
       "1  This worked very well and is EASY.  I used not...       4\n",
       "2  very very sweet. after i waited the 2 days i b...       4\n",
       "3  This recipe was OVERLY too sweet.  I would sta...       2\n",
       "4                                         Very good!       4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = []\n",
    "for review in review_rating.review:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", review.lower())\n",
    "     normalized_text.append(tokens)\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 442455\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'with', 'a', 'salad', 'cooked', 'on', 'top', 'of', 'stove', 'for', '15', 'minutes', 'added', 'a', 'shake', 'of', 'cayenne', 'and', 'a', 'pinch', 'of', 'salt', 'used', 'low', 'fat', 'sour', 'cream', 'thanks']\n",
      "['this', 'worked', 'very', 'well', 'and', 'is', 'easy', 'i', 'used', 'not', 'quite', 'a', 'whole', 'package', '10oz', 'of', 'white', 'chips', 'great']\n",
      "['very', 'very', 'sweet', 'after', 'i', 'waited', 'the', '2', 'days', 'i', 'bought', '2', 'more', 'pints', 'of', 'raspberries', 'and', 'added', 'them', 'to', 'the', 'mix', 'i', 'm', 'going', 'to', 'add', 'some', 'as', 'a', 'cake', 'filling', 'today', 'and', 'will', 'take', 'a', 'photo']\n"
     ]
    }
   ],
   "source": [
    "for line in result[:3]: # 샘플 3개만 출력\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 Word2Vec의 하이퍼파라미터값은 다음과 같습니다.\n",
    "\n",
    "size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "\n",
    "window = 컨텍스트 윈도우 크기\n",
    "\n",
    "min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "\n",
    "workers = 학습을 위한 프로세스 수\n",
    "\n",
    "sg = 0은 CBOW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chili', 0.8027944564819336), ('chilli', 0.7815027832984924), ('wasabi', 0.7604758143424988), ('amchur', 0.7525372505187988), ('chile', 0.7496981024742126), ('turmeric', 0.7213866114616394), ('asafetida', 0.7053084373474121), ('masala', 0.6654837131500244), ('tamarind', 0.6522266864776611), ('anchovy', 0.6387553215026855)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"curry\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('data/eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"data/eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('receipe', 0.8577945232391357), ('recipie', 0.8383318185806274), ('receipt', 0.6162335872650146), ('reciepe', 0.6137989163398743), ('recipee', 0.5628527402877808), ('dish', 0.5294333696365356), ('reciped', 0.5250300168991089), ('directions', 0.518814206123352), ('recepie', 0.5123901963233948), ('reicpe', 0.5064900517463684)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"recipe\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
     ]
    }
   ],
   "source": [
    "X_encoded = t.texts_to_sequences(sentences)\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_len=max(len(l) for l in X_encoded)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "X_train=pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train=np.array(y_train)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 4, input_length=max_len)) # 모든 임베딩 벡터는 4차원.\n",
    "model.add(Flatten()) # Dense의 입력으로 넣기위함.\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples\n",
      "Epoch 1/100\n",
      "7/7 - 1s - loss: 0.6853 - acc: 0.7143\n",
      "Epoch 2/100\n",
      "7/7 - 0s - loss: 0.6840 - acc: 0.7143\n",
      "Epoch 3/100\n",
      "7/7 - 0s - loss: 0.6827 - acc: 0.7143\n",
      "Epoch 4/100\n",
      "7/7 - 0s - loss: 0.6814 - acc: 0.8571\n",
      "Epoch 5/100\n",
      "7/7 - 0s - loss: 0.6802 - acc: 0.8571\n",
      "Epoch 6/100\n",
      "7/7 - 0s - loss: 0.6789 - acc: 0.8571\n",
      "Epoch 7/100\n",
      "7/7 - 0s - loss: 0.6776 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "7/7 - 0s - loss: 0.6763 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "7/7 - 0s - loss: 0.6750 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "7/7 - 0s - loss: 0.6737 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "7/7 - 0s - loss: 0.6723 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "7/7 - 0s - loss: 0.6710 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "7/7 - 0s - loss: 0.6697 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "7/7 - 0s - loss: 0.6684 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "7/7 - 0s - loss: 0.6670 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "7/7 - 0s - loss: 0.6657 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "7/7 - 0s - loss: 0.6644 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "7/7 - 0s - loss: 0.6630 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "7/7 - 0s - loss: 0.6616 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "7/7 - 0s - loss: 0.6603 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "7/7 - 0s - loss: 0.6589 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "7/7 - 0s - loss: 0.6575 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "7/7 - 0s - loss: 0.6561 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "7/7 - 0s - loss: 0.6547 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "7/7 - 0s - loss: 0.6533 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "7/7 - 0s - loss: 0.6519 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "7/7 - 0s - loss: 0.6505 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "7/7 - 0s - loss: 0.6490 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "7/7 - 0s - loss: 0.6476 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "7/7 - 0s - loss: 0.6461 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "7/7 - 0s - loss: 0.6447 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "7/7 - 0s - loss: 0.6432 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "7/7 - 0s - loss: 0.6417 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "7/7 - 0s - loss: 0.6402 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "7/7 - 0s - loss: 0.6387 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "7/7 - 0s - loss: 0.6372 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "7/7 - 0s - loss: 0.6357 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "7/7 - 0s - loss: 0.6342 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "7/7 - 0s - loss: 0.6326 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "7/7 - 0s - loss: 0.6311 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "7/7 - 0s - loss: 0.6295 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "7/7 - 0s - loss: 0.6279 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "7/7 - 0s - loss: 0.6264 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "7/7 - 0s - loss: 0.6248 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "7/7 - 0s - loss: 0.6232 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "7/7 - 0s - loss: 0.6216 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "7/7 - 0s - loss: 0.6200 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "7/7 - 0s - loss: 0.6183 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "7/7 - 0s - loss: 0.6167 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "7/7 - 0s - loss: 0.6150 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "7/7 - 0s - loss: 0.6134 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "7/7 - 0s - loss: 0.6117 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "7/7 - 0s - loss: 0.6100 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "7/7 - 0s - loss: 0.6084 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "7/7 - 0s - loss: 0.6067 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "7/7 - 0s - loss: 0.6050 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "7/7 - 0s - loss: 0.6032 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "7/7 - 0s - loss: 0.6015 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "7/7 - 0s - loss: 0.5998 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "7/7 - 0s - loss: 0.5980 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "7/7 - 0s - loss: 0.5963 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "7/7 - 0s - loss: 0.5945 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "7/7 - 0s - loss: 0.5928 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "7/7 - 0s - loss: 0.5910 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "7/7 - 0s - loss: 0.5892 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "7/7 - 0s - loss: 0.5874 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "7/7 - 0s - loss: 0.5856 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "7/7 - 0s - loss: 0.5838 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "7/7 - 0s - loss: 0.5820 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "7/7 - 0s - loss: 0.5801 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "7/7 - 0s - loss: 0.5783 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "7/7 - 0s - loss: 0.5764 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "7/7 - 0s - loss: 0.5746 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "7/7 - 0s - loss: 0.5727 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "7/7 - 0s - loss: 0.5708 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "7/7 - 0s - loss: 0.5690 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "7/7 - 0s - loss: 0.5671 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "7/7 - 0s - loss: 0.5652 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "7/7 - 0s - loss: 0.5633 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "7/7 - 0s - loss: 0.5614 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "7/7 - 0s - loss: 0.5595 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "7/7 - 0s - loss: 0.5575 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "7/7 - 0s - loss: 0.5556 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "7/7 - 0s - loss: 0.5537 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "7/7 - 0s - loss: 0.5517 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "7/7 - 0s - loss: 0.5498 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "7/7 - 0s - loss: 0.5478 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "7/7 - 0s - loss: 0.5458 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "7/7 - 0s - loss: 0.5439 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "7/7 - 0s - loss: 0.5419 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "7/7 - 0s - loss: 0.5399 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "7/7 - 0s - loss: 0.5379 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "7/7 - 0s - loss: 0.5359 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "7/7 - 0s - loss: 0.5339 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "7/7 - 0s - loss: 0.5319 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "7/7 - 0s - loss: 0.5299 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "7/7 - 0s - loss: 0.5279 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "7/7 - 0s - loss: 0.5259 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "7/7 - 0s - loss: 0.5239 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "7/7 - 0s - loss: 0.5218 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f5c470fe80>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '-0.038194', '-0.24487', '0.72812', '-0.39961', '0.083172', '0.043953', '-0.39141', '0.3344', '-0.57545', '0.087459', '0.28787', '-0.06731', '0.30906', '-0.26384', '-0.13231', '-0.20757', '0.33395', '-0.33848', '-0.31743', '-0.48336', '0.1464', '-0.37304', '0.34577', '0.052041', '0.44946', '-0.46971', '0.02628', '-0.54155', '-0.15518', '-0.14107', '-0.039722', '0.28277', '0.14393', '0.23464', '-0.31021', '0.086173', '0.20397', '0.52624', '0.17164', '-0.082378', '-0.71787', '-0.41531', '0.20335', '-0.12763', '0.41367', '0.55187', '0.57908', '-0.33477', '-0.36559', '-0.54857', '-0.062892', '0.26584', '0.30205', '0.99775', '-0.80481', '-3.0243', '0.01254', '-0.36942', '2.2167', '0.72201', '-0.24978', '0.92136', '0.034514', '0.46745', '1.1079', '-0.19358', '-0.074575', '0.23353', '-0.052062', '-0.22044', '0.057162', '-0.15806', '-0.30798', '-0.41625', '0.37972', '0.15006', '-0.53212', '-0.2055', '-1.2526', '0.071624', '0.70565', '0.49744', '-0.42063', '0.26148', '-1.538', '-0.30223', '-0.073438', '-0.28312', '0.37104', '-0.25217', '0.016215', '-0.017099', '-0.38984', '0.87424', '-0.72569', '-0.51058', '-0.52028', '-0.1459', '0.8278', '0.27062']\n",
      "the\n",
      "[',', '-0.10767', '0.11053', '0.59812', '-0.54361', '0.67396', '0.10663', '0.038867', '0.35481', '0.06351', '-0.094189', '0.15786', '-0.81665', '0.14172', '0.21939', '0.58505', '-0.52158', '0.22783', '-0.16642', '-0.68228', '0.3587', '0.42568', '0.19021', '0.91963', '0.57555', '0.46185', '0.42363', '-0.095399', '-0.42749', '-0.16567', '-0.056842', '-0.29595', '0.26037', '-0.26606', '-0.070404', '-0.27662', '0.15821', '0.69825', '0.43081', '0.27952', '-0.45437', '-0.33801', '-0.58184', '0.22364', '-0.5778', '-0.26862', '-0.20425', '0.56394', '-0.58524', '-0.14365', '-0.64218', '0.0054697', '-0.35248', '0.16162', '1.1796', '-0.47674', '-2.7553', '-0.1321', '-0.047729', '1.0655', '1.1034', '-0.2208', '0.18669', '0.13177', '0.15117', '0.7131', '-0.35215', '0.91348', '0.61783', '0.70992', '0.23955', '-0.14571', '-0.37859', '-0.045959', '-0.47368', '0.2385', '0.20536', '-0.18996', '0.32507', '-1.1112', '-0.36341', '0.98679', '-0.084776', '-0.54008', '0.11726', '-1.0194', '-0.24424', '0.12771', '0.013884', '0.080374', '-0.35414', '0.34951', '-0.7226', '0.37549', '0.4441', '-0.99059', '0.61214', '-0.35111', '-0.83155', '0.45293', '0.082577']\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "f = open('data/glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split() # 각 줄을 읽어와서 word_vector에 저장.\n",
    "    print(word_vector) # 각 줄을 출력\n",
    "    word = word_vector[0] # word_vector에서 첫번째 값만 저장\n",
    "    print(word) # word_vector의 첫번째 값만 출력\n",
    "    n=n+1\n",
    "    if n==2:\n",
    "        break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(type(word_vector))\n",
    "print(len(word_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000개의 Embedding vector가 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embedding_dict = dict()\n",
    "f = open('data/glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
      "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
      "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
      "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
      " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
      " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
      " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
      " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
      " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
      " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
      " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
      "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
      "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
      "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
      " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
      " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
      " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(embedding_dict['respectable'])\n",
    "print(len(embedding_dict['respectable']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 100)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "# 단어 집합 크기의 행과 100개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in t.word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.\n",
    "    temp = embedding_dict.get(word) # 단어(key) 해당되는 임베딩 벡터의 100개의 값(value)를 임시 변수에 저장\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp # 임수 변수의 값을 단어와 맵핑되는 인덱스의 행에 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples\n",
      "Epoch 1/100\n",
      "7/7 - 0s - loss: 0.5711 - acc: 0.5714\n",
      "Epoch 2/100\n",
      "7/7 - 0s - loss: 0.5551 - acc: 0.5714\n",
      "Epoch 3/100\n",
      "7/7 - 0s - loss: 0.5396 - acc: 0.5714\n",
      "Epoch 4/100\n",
      "7/7 - 0s - loss: 0.5246 - acc: 0.7143\n",
      "Epoch 5/100\n",
      "7/7 - 0s - loss: 0.5102 - acc: 0.8571\n",
      "Epoch 6/100\n",
      "7/7 - 0s - loss: 0.4963 - acc: 0.8571\n",
      "Epoch 7/100\n",
      "7/7 - 0s - loss: 0.4828 - acc: 0.8571\n",
      "Epoch 8/100\n",
      "7/7 - 0s - loss: 0.4698 - acc: 0.8571\n",
      "Epoch 9/100\n",
      "7/7 - 0s - loss: 0.4572 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "7/7 - 0s - loss: 0.4451 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "7/7 - 0s - loss: 0.4333 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "7/7 - 0s - loss: 0.4219 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "7/7 - 0s - loss: 0.4110 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "7/7 - 0s - loss: 0.4003 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "7/7 - 0s - loss: 0.3900 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "7/7 - 0s - loss: 0.3801 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "7/7 - 0s - loss: 0.3704 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "7/7 - 0s - loss: 0.3611 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "7/7 - 0s - loss: 0.3520 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "7/7 - 0s - loss: 0.3432 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "7/7 - 0s - loss: 0.3347 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "7/7 - 0s - loss: 0.3265 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "7/7 - 0s - loss: 0.3185 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "7/7 - 0s - loss: 0.3108 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "7/7 - 0s - loss: 0.3033 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "7/7 - 0s - loss: 0.2960 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "7/7 - 0s - loss: 0.2890 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "7/7 - 0s - loss: 0.2822 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "7/7 - 0s - loss: 0.2755 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "7/7 - 0s - loss: 0.2691 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "7/7 - 0s - loss: 0.2629 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "7/7 - 0s - loss: 0.2569 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "7/7 - 0s - loss: 0.2511 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "7/7 - 0s - loss: 0.2454 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "7/7 - 0s - loss: 0.2399 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "7/7 - 0s - loss: 0.2346 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "7/7 - 0s - loss: 0.2295 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "7/7 - 0s - loss: 0.2245 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "7/7 - 0s - loss: 0.2196 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "7/7 - 0s - loss: 0.2149 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "7/7 - 0s - loss: 0.2104 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "7/7 - 0s - loss: 0.2059 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "7/7 - 0s - loss: 0.2016 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "7/7 - 0s - loss: 0.1975 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "7/7 - 0s - loss: 0.1934 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "7/7 - 0s - loss: 0.1895 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "7/7 - 0s - loss: 0.1857 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "7/7 - 0s - loss: 0.1820 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "7/7 - 0s - loss: 0.1785 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "7/7 - 0s - loss: 0.1750 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "7/7 - 0s - loss: 0.1716 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "7/7 - 0s - loss: 0.1683 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "7/7 - 0s - loss: 0.1651 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "7/7 - 0s - loss: 0.1621 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "7/7 - 0s - loss: 0.1590 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "7/7 - 0s - loss: 0.1561 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "7/7 - 0s - loss: 0.1533 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "7/7 - 0s - loss: 0.1505 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "7/7 - 0s - loss: 0.1478 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "7/7 - 0s - loss: 0.1452 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "7/7 - 0s - loss: 0.1427 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "7/7 - 0s - loss: 0.1402 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "7/7 - 0s - loss: 0.1378 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "7/7 - 0s - loss: 0.1354 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "7/7 - 0s - loss: 0.1332 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "7/7 - 0s - loss: 0.1309 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "7/7 - 0s - loss: 0.1288 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "7/7 - 0s - loss: 0.1267 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "7/7 - 0s - loss: 0.1246 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "7/7 - 0s - loss: 0.1226 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "7/7 - 0s - loss: 0.1207 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "7/7 - 0s - loss: 0.1188 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "7/7 - 0s - loss: 0.1169 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "7/7 - 0s - loss: 0.1151 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "7/7 - 0s - loss: 0.1133 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "7/7 - 0s - loss: 0.1116 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "7/7 - 0s - loss: 0.1099 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "7/7 - 0s - loss: 0.1083 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "7/7 - 0s - loss: 0.1067 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "7/7 - 0s - loss: 0.1051 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "7/7 - 0s - loss: 0.1036 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "7/7 - 0s - loss: 0.1021 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "7/7 - 0s - loss: 0.1007 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "7/7 - 0s - loss: 0.0992 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "7/7 - 0s - loss: 0.0978 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "7/7 - 0s - loss: 0.0965 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "7/7 - 0s - loss: 0.0952 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "7/7 - 0s - loss: 0.0938 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "7/7 - 0s - loss: 0.0926 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "7/7 - 0s - loss: 0.0913 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "7/7 - 0s - loss: 0.0901 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "7/7 - 0s - loss: 0.0889 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "7/7 - 0s - loss: 0.0878 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "7/7 - 0s - loss: 0.0866 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "7/7 - 0s - loss: 0.0855 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "7/7 - 0s - loss: 0.0844 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "7/7 - 0s - loss: 0.0833 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "7/7 - 0s - loss: 0.0823 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "7/7 - 0s - loss: 0.0813 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "7/7 - 0s - loss: 0.0803 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f5c31da630>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# 구글의 사전 훈련된 Word2vec 모델을 로드합니다.\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.vectors.shape) # 모델의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 300)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "# 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples\n",
      "Epoch 1/100\n",
      "7/7 - 1s - loss: 0.6928 - acc: 0.4286\n",
      "Epoch 2/100\n",
      "7/7 - 0s - loss: 0.6740 - acc: 0.4286\n",
      "Epoch 3/100\n",
      "7/7 - 0s - loss: 0.6558 - acc: 0.5714\n",
      "Epoch 4/100\n",
      "7/7 - 0s - loss: 0.6382 - acc: 0.7143\n",
      "Epoch 5/100\n",
      "7/7 - 0s - loss: 0.6211 - acc: 0.8571\n",
      "Epoch 6/100\n",
      "7/7 - 0s - loss: 0.6045 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "7/7 - 0s - loss: 0.5886 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "7/7 - 0s - loss: 0.5731 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "7/7 - 0s - loss: 0.5583 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "7/7 - 0s - loss: 0.5439 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "7/7 - 0s - loss: 0.5300 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "7/7 - 0s - loss: 0.5167 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "7/7 - 0s - loss: 0.5038 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "7/7 - 0s - loss: 0.4913 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "7/7 - 0s - loss: 0.4793 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "7/7 - 0s - loss: 0.4678 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "7/7 - 0s - loss: 0.4566 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "7/7 - 0s - loss: 0.4458 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "7/7 - 0s - loss: 0.4353 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "7/7 - 0s - loss: 0.4252 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "7/7 - 0s - loss: 0.4154 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "7/7 - 0s - loss: 0.4060 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "7/7 - 0s - loss: 0.3968 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "7/7 - 0s - loss: 0.3880 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "7/7 - 0s - loss: 0.3794 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "7/7 - 0s - loss: 0.3711 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "7/7 - 0s - loss: 0.3630 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "7/7 - 0s - loss: 0.3552 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "7/7 - 0s - loss: 0.3476 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "7/7 - 0s - loss: 0.3402 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "7/7 - 0s - loss: 0.3331 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "7/7 - 0s - loss: 0.3262 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "7/7 - 0s - loss: 0.3195 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "7/7 - 0s - loss: 0.3129 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "7/7 - 0s - loss: 0.3066 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "7/7 - 0s - loss: 0.3005 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "7/7 - 0s - loss: 0.2945 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "7/7 - 0s - loss: 0.2887 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "7/7 - 0s - loss: 0.2830 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "7/7 - 0s - loss: 0.2776 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "7/7 - 0s - loss: 0.2722 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "7/7 - 0s - loss: 0.2670 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "7/7 - 0s - loss: 0.2620 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "7/7 - 0s - loss: 0.2571 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "7/7 - 0s - loss: 0.2524 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "7/7 - 0s - loss: 0.2477 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "7/7 - 0s - loss: 0.2432 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "7/7 - 0s - loss: 0.2388 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "7/7 - 0s - loss: 0.2345 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "7/7 - 0s - loss: 0.2304 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "7/7 - 0s - loss: 0.2263 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "7/7 - 0s - loss: 0.2224 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "7/7 - 0s - loss: 0.2185 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "7/7 - 0s - loss: 0.2148 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "7/7 - 0s - loss: 0.2112 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "7/7 - 0s - loss: 0.2076 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "7/7 - 0s - loss: 0.2041 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "7/7 - 0s - loss: 0.2008 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "7/7 - 0s - loss: 0.1975 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "7/7 - 0s - loss: 0.1942 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "7/7 - 0s - loss: 0.1911 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "7/7 - 0s - loss: 0.1881 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "7/7 - 0s - loss: 0.1851 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "7/7 - 0s - loss: 0.1822 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "7/7 - 0s - loss: 0.1793 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "7/7 - 0s - loss: 0.1765 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "7/7 - 0s - loss: 0.1738 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "7/7 - 0s - loss: 0.1712 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "7/7 - 0s - loss: 0.1686 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "7/7 - 0s - loss: 0.1661 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "7/7 - 0s - loss: 0.1636 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "7/7 - 0s - loss: 0.1612 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "7/7 - 0s - loss: 0.1588 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "7/7 - 0s - loss: 0.1565 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "7/7 - 0s - loss: 0.1543 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "7/7 - 0s - loss: 0.1521 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "7/7 - 0s - loss: 0.1499 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "7/7 - 0s - loss: 0.1478 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "7/7 - 0s - loss: 0.1458 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "7/7 - 0s - loss: 0.1437 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "7/7 - 0s - loss: 0.1418 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "7/7 - 0s - loss: 0.1398 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "7/7 - 0s - loss: 0.1379 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "7/7 - 0s - loss: 0.1361 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "7/7 - 0s - loss: 0.1343 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "7/7 - 0s - loss: 0.1325 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "7/7 - 0s - loss: 0.1308 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "7/7 - 0s - loss: 0.1291 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "7/7 - 0s - loss: 0.1274 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "7/7 - 0s - loss: 0.1258 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "7/7 - 0s - loss: 0.1242 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "7/7 - 0s - loss: 0.1226 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "7/7 - 0s - loss: 0.1210 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "7/7 - 0s - loss: 0.1195 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "7/7 - 0s - loss: 0.1181 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "7/7 - 0s - loss: 0.1166 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "7/7 - 0s - loss: 0.1152 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "7/7 - 0s - loss: 0.1138 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "7/7 - 0s - loss: 0.1124 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "7/7 - 0s - loss: 0.1111 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f58feabf98>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "# import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-aa0033cfe50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 세션 초기화. 이는 텐서플로우 개념.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0melmo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://tfhub.dev/google/elmo/1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# 텐서플로우 허브로부터 ELMo를 다운로드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jfhdz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_hub\\module.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[0;32m    174\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m           tags=self._tags)\n\u001b[0m\u001b[0;32m    177\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jfhdz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[1;34m(self, name, trainable, tags)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_variables_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jfhdz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;31m# TPU training code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mscope_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jfhdz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_init_state\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mvariable_tensor_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_state_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m     self._variable_map = recover_partitioned_variable_map(\n\u001b[0;32m    450\u001b[0m         get_node_map_from_tensor_map(variable_tensor_map))\n",
      "\u001b[1;32mc:\\users\\jfhdz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_create_state_graph\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m         import_scope=relative_scope_name)\n\u001b[0m\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[1;31m# Build a list from the variable name in the module definition to the actual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jfhdz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   1451\u001b[0m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[0;32m   1452\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1453\u001b[1;33m                                                  **kwargs)[0]\n\u001b[0m\u001b[0;32m   1454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jfhdz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m   \u001b[1;34m\"\"\"Import MetaGraph, and return both a saver and returned elements.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1463\u001b[1;33m     raise RuntimeError(\"Exporting/importing meta graphs is not supported when \"\n\u001b[0m\u001b[0;32m   1464\u001b[0m                        \u001b[1;34m\"eager execution is enabled. No graph exists when eager \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m                        \"execution is enabled.\")\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "# 세션 초기화. 이는 텐서플로우 개념.\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
    "# 텐서플로우 허브로부터 ELMo를 다운로드\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
